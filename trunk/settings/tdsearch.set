#-----------------------------------------------------------------------------
# Temporal-difference search using local shapes in a short-term memory
#-----------------------------------------------------------------------------

### MAIN ###

Object = RlSetup
{
    ID = Setup
    Version = 7
    InputPath = input
    OutputPath = output/tdsearch
    BoardSize = 9
    Rules = cgos
    RandomSeed = 1 # 0 means randomise, -1 means use default
    Process = -1 # Single-process
    SelfPlay = NULL # Override with SelfPlay to run in self-test mode
    DebugOutput = 1
    DebugLevel = 2
    Verification = 0
    DefaultTime = 1
    MainAgent = MainAgent
    SimAgent = SimAgent
    BookFile = NULL
    NumGtp = 0
}

# For running games of self-play in test mode (not used for search)
Object = RlSimulator
{
    ID = SelfPlay 
    Version = 13
    Agent = MainAgent
    ControlMode = 0 # MaxGames
    TimeControl = NULL
    MaxGames = 1000 # Main parameter specifying simulations per move
    Truncate = -1 # Never truncate
    Resign = 0 # Never resign
    DefaultPolicy = NULL
    FuegoPlayout = NULL
    MaxSimMoves = 1000 # Abandon simulations that exceed this length
    FastReset = 0 # Never use fast reset for real agent
    Log = 0 # Log data about simulations
    Record = 0 # Log game records from simulations
    Pondering = 0 # Think on opponent time
}

### AGENTS ###

Object = RlRealAgent
{
    ID = MainAgent
    Version = 22
    Policy = Greedy
    Evaluator = Evaluator
    FeatureSet = LocalShapeSet
    WeightSet = WeightSet
    History = History
    Trainer = NULL
    Log = MainLog
    ResignThreshold = 0
    Prune = 0
    Simulator = Simulator
    WeightFile = NULL
    ResetWeights = 2 # Reset on new game
    MinWeight = 0 [ ]
    MaxWeight = 0 [ ]
    AnswersAll = 1 # Only change answers for predictive RLGO
}

Object = RlSimAgent
{
    ID = SimAgent
    Version = 22
    Policy = TwoStage
    Evaluator = SimEvaluator
    FeatureSet = LocalShapeSet
    WeightSet = WeightSet
    History = SimHistory
    Trainer = CurrentTrainer
    Log = SimLog
    ResignThreshold = 0
    Prune = 0
}

Object = RlAgentLog
{
    ID = MainLog
    Version = 3
    Agent = MainAgent
    DebugLevel = 2
    LogMode = 1
    Interval = 1
    IntervalMul = 0
    LogStartOnly = 0
    SaveRecord = 0
    SaveWeights = 0
    TopTex = 0
    LiveGraphics = 0
    TraceFeatures = NULL
    Pause = 0
    Policy = Greedy
    NumPV = 8
    NumBest = 4
}

Object = RlAgentLog
{
    ID = SimLog
    Version = 3
    Agent = SimAgent
    DebugLevel = 2
    LogMode = 0
    Interval = 1000
    IntervalMul = 0
    LogStartOnly = 1
    SaveRecord = 0
    SaveWeights = 0
    TopTex = 0
    LiveGraphics = 0
    TraceFeatures = NULL
    Pause = 0
    Policy = SimGreedy
    NumPV = 8
    NumBest = 4
}

Object = RlSimulator
{
    ID = Simulator
    Version = 13
    Agent = SimAgent
    ControlMode = 0 # MaxGames
    TimeControl = NULL
    MaxGames = 1000 # Main parameter specifying simulations per move
    Truncate = -1
    Resign = 1
    DefaultPolicy = FuegoOnPolicy
    FuegoPlayout = FuegoPlayout
    MaxSimMoves = 200 # Abandon simulations that exceed this length
    FastReset = 1
    Log = 0 # Log data about simulations
    Record = 0 # Log game records from simulations
    Pondering = 0 # Think on opponent time
}

Object = RlHistory
{
    ID = History
    Capacity = 100
}

Object = RlHistory
{
    ID = SimHistory
    Capacity = 100
}

### FEATURES ###

Object = RlWeightSet
{
    ID = WeightSet
    FeatureSet = LocalShapeSet
    ShareName = NULL
    Strict = 1
    StreamMode = 1 # StreamValue
}

Object = RlEvaluator
{
    ID = Evaluator
    Version = 6
    FeatureSet = LocalShapeSet
    WeightSet = WeightSet
    MoveFilter = SimpleEyes
    Differences = 0
    SupportUndo = 1
}

Object = RlEvaluator
{
    ID = SimEvaluator
    Version = 6
    FeatureSet = LocalShapeSet
    WeightSet = WeightSet
    MoveFilter = SimSimpleEyes
    Differences = 1
    SupportUndo = 0
}

Object = RlLocalShapeSet
{
    ID = LocalShapeSet
    Version = 14
    ShapeSpec = SQUARE
    MinSize = 1
    MaxSize = 3
    ShareTypes = 1 [ None ]
    IgnoreEmpty = 1
    IgnoreSelfInverse = 0
}

### POLICIES ###

Object = RlRandomPolicy
{
    ID = Random
    Version = 1
    Evaluator = SimEvaluator
    OnPolicy = 1
}

Object = RlEpsilonPolicy
{
    ID = Epsilon
    Version = 3
    PPolicy = Random
    NPolicy = SimGreedy
    Epsilon = 0.1
}

Object = RlGreedy
{
    ID = Greedy
    Version = 1
    Evaluator = Evaluator
    History = History
}

Object = RlGreedy
{
    ID = SimGreedy
    Version = 1
    Evaluator = SimEvaluator
    History = SimHistory
}

Object = RlFuegoPlayout
{
    ID = FuegoPlayout
}

Object = RlFuegoPlayoutPolicy
{
    ID = FuegoOnPolicy
    OnPolicy = 1
    FuegoPlayout = FuegoPlayout
    Incremental = 1
}

Object = RlTwoStagePolicy
{
    ID = TwoStage
    Version = 2
    Policy1 = Epsilon
    Policy2 = FuegoOnPolicy
    SwitchTime = 6
}

### MOVE FILTERS ###

Object = RlSimpleEyeFilter
{
    ID = SimpleEyes
    ConsiderPass = 0
}

Object = RlSimpleEyeFilter
{
    ID = SimSimpleEyes
    ConsiderPass = 0
}

### LEARNING ###

Object = RlForwardTrainer
{
    ID = CurrentTrainer
    Version = 0
    LearningRule = TD0
    History = SimHistory
    Evaluator = SimEvaluator
    Episodes = 0 #Current
    NumReplays = 1
    UpdateRoot = 1
    TemporalDifference = 2
    RefreshValues = 0
    Interleave = 1
}

Object = RlTD0
{
    ID = TD0
    Version = 1
    WeightSet = WeightSet
    Alpha = 0.1
    StepSizeMode = 2
    UseOffPolicy = 1
    Logistic = 1
    MSE = 0
    MinGrad = 0
    Log = SimLog
}

### END ###
